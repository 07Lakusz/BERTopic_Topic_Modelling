{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7kMdVjzI+8z8/1Kz/G+wj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/07Lakusz/BERTopic_Topic_Modelling/blob/main/BERTopic_Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic Pipeline\n",
        "## Overview of the Methodology\n",
        "This notebook implements a sophisticated topic modeling pipeline using the BERTopic library. The process begins by ingesting bibliographic data from `.ris` files, extracting abstracts, and breaking them down into individual sentences. These sentences, the fundamental units of analysis, are then transformed into high-dimensional numerical vectors (`embeddings`) using a pre-trained `SentenceTransformer` model.\n",
        "To make these embeddings clusterable, their dimensionality is reduced using `UMAP` (Uniform Manifold Approximation and Projection), an algorithm that preserves the data's topological structure. Subsequently, `HDBSCAN` (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is applied to the reduced-dimension embeddings to identify dense clusters of semantically similar sentences. Each cluster represents a potential topic.\n",
        "Finally, a class-based `TF-IDF` (c-TF-IDF) algorithm is used to extract the most representative keywords for each topic, providing human-interpretable labels. The pipeline also includes optional steps for hierarchical topic reduction and re-assigning outlier sentences to existing topics, enhancing the coherence and comprehensiveness of the final model. The results, including topic assignments, summaries, and various visualizations, are generated and made available for download."
      ],
      "metadata": {
        "id": "KrgAIJMifFW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRJ7VAuiVwep"
      },
      "outputs": [],
      "source": [
        "# @title 1. Installation and Imports\n",
        "# @markdown Run this cell to install and import all required libraries. <br> This cell prepares the Colab environment. It begins by installing the necessary Python libraries that are not included by default, such as bertopic and rispy. After installation, it imports all the required modules for data handling, natural language processing, topic modeling, and plotting. Finally, it downloads the essential NLTK resources (stopwords, punkt, and punkt_tab) which are required for text preprocessing in the later cells.\n",
        "\n",
        "# --- Install necessary packages ---\n",
        "# Using --quiet to keep the output clean.\n",
        "!pip install rispy bertopic sentence-transformers umap-learn hdbscan --quiet\n",
        "\n",
        "# --- Import Core Libraries ---\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# --- Data Handling and Scientific Computing ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Bibliographic Data Parsing ---\n",
        "import rispy\n",
        "\n",
        "# --- Natural Language Processing ---\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# --- Topic Modeling and Machine Learning ---\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "import hdbscan\n",
        "import torch\n",
        "\n",
        "# --- Plotting ---\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Download NLTK resources for text processing ---\n",
        "# These resources are used for tokenizing text into sentences and removing common stopwords.\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "print(\"✅ Cell 1 complete: All libraries installed, imported, and NLTK resources are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Settings Configuration\n",
        "# @markdown Adjust the parameters below to customize the topic modeling process. <br> This cell contains a single Python dictionary named SETTINGS. This centralized configuration allows for easy modification of the topic modeling pipeline's parameters without altering the core logic. Each setting is documented to explain its purpose. Key parameters include the choice of embedding model, configurations for dimensionality reduction (UMAP) and clustering (HDBSCAN), and options for post-processing steps.\n",
        "\n",
        "SETTINGS = {\n",
        "    # --- General Settings ---\n",
        "    \"EMBEDDING_MODEL_NAME\": \"all-MiniLM-L6-v2\",  # Model from SentenceTransformers to create numerical representations (embeddings) of sentences.\n",
        "    \"DEVICE\": None,  # Set to \"cuda\", \"cpu\", or None to auto-detect. Using a GPU (\"cuda\") is highly recommended for speed.\n",
        "    \"EMBEDDING_BATCH_SIZE\": 32,  # Number of sentences to process at once during embedding. Higher values may be faster but require more GPU memory.\n",
        "    \"RANDOM_STATE\": 42,  # A fixed seed for random processes to ensure that results are reproducible each time the code is run.\n",
        "    \"VERBOSE\": True,  # If True, BERTopic will print progress updates during the analysis.\n",
        "    \"CALCULATE_PROBABILITIES\": True,  # If True, calculates the probability of each sentence belonging to each topic. Useful for analysis but can increase computation time.\n",
        "\n",
        "    # --- UMAP (Dimensionality Reduction) Settings ---\n",
        "    # UMAP reduces the high-dimensional embedding space to a lower dimension, making clustering feasible and effective.\n",
        "    \"UMAP_N_NEIGHBORS\": 20,  # Controls how UMAP balances local versus global structure in the data. Higher values focus more on the overall structure.\n",
        "    \"UMAP_N_COMPONENTS\": 5,  # The dimension of the data after reduction. 5 is a common default for BERTopic.\n",
        "    \"UMAP_MIN_DIST\": 0.0,  # Controls how tightly UMAP packs points together. Lower values create more distinct and dense clusters.\n",
        "    \"UMAP_METRIC\": 'cosine',  # The distance metric used for the embedding vectors. Cosine similarity is standard for text data.\n",
        "\n",
        "    # --- HDBSCAN (Clustering) Settings ---\n",
        "    # HDBSCAN is a density-based algorithm that identifies clusters of sentences (i.e., topics) in the reduced embedding space.\n",
        "    \"HDBSCAN_MIN_CLUSTER_SIZE\": 10,  # The minimum number of sentences required to form a distinct topic. This is a critical parameter to control the number of topics.\n",
        "    \"HDBSCAN_METRIC\": 'euclidean',  # The distance metric used on the UMAP-reduced data.\n",
        "    \"HDBSCAN_CLUSTER_SELECTION_METHOD\": 'eom',  # 'eom' (Excess of Mass) is a robust algorithm for selecting the most stable clusters.\n",
        "\n",
        "    # --- Topic Reduction Settings ---\n",
        "    \"AUTO_REDUCE_TOPICS\": True,  # If True, automatically merges similar topics to create a more consolidated and interpretable set.\n",
        "    \"TOPIC_REDUCTION_NR_TOPICS\": 'auto',  # Can be 'auto' for automatic reduction or a specific integer (e.g., 20) to define the final number of topics.\n",
        "\n",
        "    # --- Post-processing to Reduce Outliers ---\n",
        "    \"REDUCE_OUTLIERS_POST_PROCESSING\": True, # If True, attempts to assign outlier sentences (those not assigned to any topic) to the nearest topic cluster.\n",
        "    \"OUTLIER_REDUCTION_STRATEGY\": \"c-tf-idf\", # The method for reassigning outliers. \"c-tf-idf\" is a robust choice that considers topic keyword relevance.\n",
        "\n",
        "    # --- CountVectorizer (Topic Representation) Settings ---\n",
        "    # This module extracts candidate keywords for each topic based on word frequency.\n",
        "    \"CV_STOP_WORDS\": \"english\",  # Removes common, non-informative English words (e.g., \"the\", \"a\", \"is\").\n",
        "    \"CV_NGRAM_RANGE\": (1, 2),  # Considers single words (unigrams) and two-word phrases (bigrams) as potential keywords.\n",
        "    \"CV_MIN_DF\": 5,  # A word or phrase must appear in at least 5 different sentences to be considered a potential keyword. Helps filter out rare noise.\n",
        "    \"CV_MAX_FEATURES\": None,  # Maximum number of keywords to consider across all topics. `None` means no limit.\n",
        "\n",
        "    # --- Representation Model Settings ---\n",
        "    # These models refine the keywords selected by CountVectorizer to create better, more coherent topic labels.\n",
        "    \"REPRESENTATION_MODELS\": [\n",
        "        KeyBERTInspired(),  # Uses a BERT-based model to find keywords that are highly relevant to the topic's documents.\n",
        "        MaximalMarginalRelevance(diversity=0.3),  # Diversifies the selected keywords to avoid redundancy and improve interpretability.\n",
        "    ],\n",
        "\n",
        "    # --- Output File Names ---\n",
        "    \"OUTPUT_CSV_TOPICS\": \"sentence_topics.csv\",\n",
        "    \"OUTPUT_CSV_SUMMARY\": \"topic_summary_table.csv\",\n",
        "    \"OUTPUT_MODEL_PATH\": \"bertopic_model\",\n",
        "    \"OUTPUT_PLOT_TOPIC_BARCHART\": \"topic_frequencies_barchart_interactive.html\",\n",
        "    \"OUTPUT_PLOT_TOPICS_2D\": \"topics_2d_visualization.html\",\n",
        "    \"OUTPUT_PLOT_TOPIC_HIERARCHY\": \"topic_hierarchy_visualization.html\",\n",
        "    \"OUTPUT_PLOT_KEYWORDS_PREFIX\": \"topic_keywords_id\",\n",
        "    \"OUTPUT_CSV_OUTLIERS\": \"outlier_sentences_before_reduction.csv\",\n",
        "    \"OUTPUT_CSV_HIERARCHY\": \"topic_hierarchy_tree.csv\",\n",
        "\n",
        "    # --- Plotting Settings ---\n",
        "    \"PLOT_TOP_N_TOPICS_KEYWORDS\": 10, # Generate individual keyword plots for the top N most frequent topics.\n",
        "    \"PLOT_NUM_KEYWORDS_PER_TOPIC\": 10, # Number of keywords to display in each individual topic plot.\n",
        "    \"CUSTOM_LABEL_NUM_KEYWORDS\": 3, # Number of keywords to use when generating the short custom topic labels (e.g., \"Topic 1: word1, word2, word3\").\n",
        "}\n",
        "\n",
        "print(\"✅ Cell 2 complete: Settings are configured and ready.\")"
      ],
      "metadata": {
        "id": "IvxHKKydV1Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Upload RIS Files and Run Analysis\n",
        "# @markdown This cell contains all functions and executes the main analysis pipeline. <br> This is the main execution cell. It defines and calls all the functions necessary to go from raw .ris files to a fully trained topic model with associated visualizations and data files. When you run this cell, it will first prompt you to upload your .ris file(s). The analysis will start automatically and may take several minutes depending on the data size.\n",
        "# @markdown 1.  **Run the cell.**\n",
        "# @markdown 2.  A file upload button will appear. **Select and upload your `.ris` file(s).**\n",
        "# @markdown 3.  The analysis will start automatically after the upload is complete. The process may take several minutes depending on the data size and whether a GPU is used.\n",
        "\n",
        "# --- Function Definitions ---\n",
        "\n",
        "def clean_abstract(text):\n",
        "    \"\"\"\n",
        "    Cleans a text string by removing extra whitespace and stopwords.\n",
        "    Args:\n",
        "        text (str): The input string (abstract).\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    nltk_stopwords = set(stopwords.words('english'))\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple whitespace characters into one.\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token.lower() not in nltk_stopwords]\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def extract_abstracts_from_ris(file_content, filename):\n",
        "    \"\"\"\n",
        "    Parses the content of a .ris file to extract abstracts.\n",
        "    Args:\n",
        "        file_content (str): The decoded content of the .ris file.\n",
        "        filename (str): The name of the file for logging purposes.\n",
        "    Returns:\n",
        "        list: A list of abstract strings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # rispy.loads is used as we are passing the file content directly.\n",
        "        entries = rispy.loads(file_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not parse file {filename}: {e}\")\n",
        "        return []\n",
        "    abstracts = [entry.get('abstract', '') for entry in entries if 'abstract' in entry]\n",
        "    return abstracts\n",
        "\n",
        "def collect_all_ris_sentences(uploaded_files):\n",
        "    \"\"\"\n",
        "    Processes all uploaded .ris files to extract and prepare sentences for analysis.\n",
        "    Args:\n",
        "        uploaded_files (dict): A dictionary from `files.upload()`, where keys are filenames and values are file content.\n",
        "    Returns:\n",
        "        tuple: A tuple containing (all_sentences, original_abstracts_map).\n",
        "               Returns (None, None) if no sentences are extracted.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "    original_abstracts_map = []\n",
        "    abstract_count = 0\n",
        "    for filename, content in uploaded_files.items():\n",
        "        if filename.lower().endswith(\".ris\"):\n",
        "            # The content from files.upload() is in bytes, so it must be decoded.\n",
        "            file_content_str = content.decode('utf-8', errors='ignore')\n",
        "            abstracts = extract_abstracts_from_ris(file_content_str, filename)\n",
        "            print(f\"Processing {len(abstracts)} abstracts from {filename}...\")\n",
        "            for abstract in abstracts:\n",
        "                if not abstract:\n",
        "                    continue\n",
        "                abstract_count += 1\n",
        "                cleaned_abstract = clean_abstract(abstract)\n",
        "                # Tokenize the cleaned abstract into individual sentences.\n",
        "                sentences_from_abstract = sent_tokenize(cleaned_abstract)\n",
        "                all_sentences.extend(sentences_from_abstract)\n",
        "                # Map each sentence back to its parent abstract.\n",
        "                original_abstracts_map.extend([cleaned_abstract] * len(sentences_from_abstract))\n",
        "\n",
        "    if not all_sentences:\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nTotal abstracts processed: {abstract_count}\")\n",
        "    print(f\"Total sentences extracted: {len(all_sentences)}\")\n",
        "    return all_sentences, original_abstracts_map\n",
        "\n",
        "def run_topic_modelling(documents, original_abstracts):\n",
        "    \"\"\"\n",
        "    Executes the entire BERTopic modeling pipeline based on the configured SETTINGS.\n",
        "    Args:\n",
        "        documents (list): A list of sentences to model.\n",
        "        original_abstracts (list): A list mapping each sentence back to its original abstract.\n",
        "    Returns:\n",
        "        tuple: A tuple containing the trained (topic_model, topics, probabilities).\n",
        "    \"\"\"\n",
        "    # Step 1: Initialize Embedding Model and select device (GPU/CPU)\n",
        "    print(f\"Loading embedding model ({SETTINGS['EMBEDDING_MODEL_NAME']})...\")\n",
        "    device = SETTINGS['DEVICE']\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Auto-detecting device: Using {device.upper()}\")\n",
        "    else:\n",
        "        print(f\"Using specified device: {device.upper()}\")\n",
        "    embedding_model = SentenceTransformer(SETTINGS['EMBEDDING_MODEL_NAME'], device=device)\n",
        "\n",
        "    # Step 2: Create Embeddings\n",
        "    print(f\"Encoding {len(documents)} sentences... (This may take a while)\")\n",
        "    embeddings = embedding_model.encode(\n",
        "        documents, show_progress_bar=SETTINGS['VERBOSE'], batch_size=SETTINGS['EMBEDDING_BATCH_SIZE']\n",
        "    )\n",
        "\n",
        "    # Step 3: Configure BERTopic Components from SETTINGS\n",
        "    print(\"\\nConfiguring UMAP for dimensionality reduction...\")\n",
        "    umap_model = umap.UMAP(n_neighbors=SETTINGS['UMAP_N_NEIGHBORS'], n_components=SETTINGS['UMAP_N_COMPONENTS'], min_dist=SETTINGS['UMAP_MIN_DIST'], metric=SETTINGS['UMAP_METRIC'], random_state=SETTINGS['RANDOM_STATE'])\n",
        "    print(\"Configuring HDBSCAN for clustering...\")\n",
        "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=SETTINGS['HDBSCAN_MIN_CLUSTER_SIZE'], metric=SETTINGS['HDBSCAN_METRIC'], cluster_selection_method=SETTINGS['HDBSCAN_CLUSTER_SELECTION_METHOD'], prediction_data=True)\n",
        "    print(\"Configuring CountVectorizer for keyword extraction...\")\n",
        "    vectorizer_model = CountVectorizer(stop_words=SETTINGS['CV_STOP_WORDS'], ngram_range=SETTINGS['CV_NGRAM_RANGE'], min_df=SETTINGS['CV_MIN_DF'], max_features=SETTINGS['CV_MAX_FEATURES'])\n",
        "    print(\"Configuring Representation Models for topic labeling...\")\n",
        "    representation_models = SETTINGS['REPRESENTATION_MODELS']\n",
        "\n",
        "    # Step 4: Initialize and Run BERTopic\n",
        "    print(\"\\nInitializing BERTopic model...\")\n",
        "    topic_model = BERTopic(\n",
        "        language=\"english\", embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model, representation_model=representation_models,\n",
        "        calculate_probabilities=SETTINGS['CALCULATE_PROBABILITIES'], verbose=SETTINGS['VERBOSE']\n",
        "    )\n",
        "    print(\"Running topic modeling... (This is the main computational step)\")\n",
        "    topics, probs = topic_model.fit_transform(documents, embeddings)\n",
        "    initial_topic_count = len(topic_model.get_topic_info())\n",
        "    print(f\"Found {initial_topic_count - 1} initial topics (before any reduction).\")\n",
        "\n",
        "    # Step 5: Optional Topic Reduction\n",
        "    if SETTINGS[\"AUTO_REDUCE_TOPICS\"]:\n",
        "        print(f\"\\nReducing number of topics with nr_topics='{SETTINGS['TOPIC_REDUCTION_NR_TOPICS']}'...\")\n",
        "        topic_model.reduce_topics(documents, nr_topics=SETTINGS['TOPIC_REDUCTION_NR_TOPICS'])\n",
        "        topics = topic_model.topics_\n",
        "        probs = topic_model.probabilities_\n",
        "        final_topic_count = len(topic_model.get_topic_info())\n",
        "        print(f\"Topics successfully reduced from {initial_topic_count - 1} to {final_topic_count - 1}.\")\n",
        "\n",
        "    # Step 6: Save Initial Outliers for review\n",
        "    initial_df = pd.DataFrame({'Sentence': documents, 'Topic': topics})\n",
        "    outlier_df_before_reduction = initial_df[initial_df[\"Topic\"] == -1]\n",
        "    if not outlier_df_before_reduction.empty:\n",
        "        outlier_df_before_reduction.to_csv(SETTINGS['OUTPUT_CSV_OUTLIERS'], index=False)\n",
        "        print(f\"\\nSaved {len(outlier_df_before_reduction)} outlier sentences (before reduction) to '{SETTINGS['OUTPUT_CSV_OUTLIERS']}'\")\n",
        "\n",
        "    # Step 7: Optional Outlier Reduction\n",
        "    if SETTINGS[\"REDUCE_OUTLIERS_POST_PROCESSING\"] and (np.array(topics) == -1).any():\n",
        "        print(f\"\\nReducing outliers using strategy: '{SETTINGS['OUTLIER_REDUCTION_STRATEGY']}'...\")\n",
        "        initial_outlier_count = (np.array(topics) == -1).sum()\n",
        "        new_topics = topic_model.reduce_outliers(documents=documents, topics=topics, strategy=SETTINGS[\"OUTLIER_REDUCTION_STRATEGY\"])\n",
        "        topics = new_topics\n",
        "        final_outlier_count = (np.array(topics) == -1).sum()\n",
        "        print(f\"Outliers reduced. Reassigned {initial_outlier_count - final_outlier_count} sentences. Final outliers: {final_outlier_count}\")\n",
        "\n",
        "    # Step 8: Save Final Results and Model\n",
        "    final_df = pd.DataFrame({\n",
        "        \"Sentence\": documents, \"Abstract\": original_abstracts, \"Topic\": topics\n",
        "    })\n",
        "    final_df.to_csv(SETTINGS['OUTPUT_CSV_TOPICS'], index=False)\n",
        "    print(f\"\\nSaved final topic assignments to '{SETTINGS['OUTPUT_CSV_TOPICS']}'\")\n",
        "\n",
        "    # Save the entire model to a directory.\n",
        "    model_path = SETTINGS['OUTPUT_MODEL_PATH']\n",
        "    topic_model.save(model_path, serialization=\"safetensors\")\n",
        "    print(f\"Saved BERTopic model to directory '{model_path}'\")\n",
        "    return topic_model, topics, probs\n",
        "\n",
        "def generate_custom_topic_labels(topic_model):\n",
        "    \"\"\"Generates human-readable labels for each topic and applies them to the model.\"\"\"\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    custom_labels = {}\n",
        "    num_keywords = SETTINGS['CUSTOM_LABEL_NUM_KEYWORDS']\n",
        "    for _, row in topic_info.iterrows():\n",
        "        topic_id = row['Topic']\n",
        "        if topic_id == -1:\n",
        "            custom_labels[topic_id] = \"Topic -1: Outliers\"\n",
        "        else:\n",
        "            # Get the top keywords for the topic.\n",
        "            words = [word[0] for word in topic_model.get_topic(topic_id)][:num_keywords]\n",
        "            custom_labels[topic_id] = f\"Topic {topic_id}: {', '.join(words)}\"\n",
        "    topic_model.set_topic_labels(custom_labels)\n",
        "    print(\"\\nCustom topic labels generated and set for the model.\")\n",
        "    return custom_labels\n",
        "\n",
        "def generate_summary_and_hierarchy_tables(topic_model, sentences):\n",
        "    \"\"\"Generates, prints, and saves the main topic summary and the hierarchical data.\"\"\"\n",
        "    # Generate hierarchical data first, if applicable.\n",
        "    hier_topics_data = None\n",
        "    if SETTINGS[\"AUTO_REDUCE_TOPICS\"]:\n",
        "        try:\n",
        "            print(\"\\nGenerating hierarchical topic structure...\")\n",
        "            hier_topics_data = topic_model.hierarchical_topics(sentences)\n",
        "            # Save the hierarchy table.\n",
        "            hier_topics_data.to_csv(SETTINGS['OUTPUT_CSV_HIERARCHY'], index=False)\n",
        "            print(f\"Saved detailed topic hierarchy tree to '{SETTINGS['OUTPUT_CSV_HIERARCHY']}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate or save topic hierarchy tree. Error: {e}\")\n",
        "\n",
        "    # Generate the main summary table.\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    df_summary = topic_info[[\"Topic\", \"Count\", \"Name\"]]\n",
        "    df_summary.columns = [\"Topic\", \"Count (Sentences)\", \"Top Words (Label)\"]\n",
        "    print(\"\\n=== Topic Summary Table ===\")\n",
        "    print(df_summary.to_string(index=False))\n",
        "    df_summary.to_csv(SETTINGS['OUTPUT_CSV_SUMMARY'], index=False)\n",
        "    print(f\"\\nSaved summary table to '{SETTINGS['OUTPUT_CSV_SUMMARY']}'\")\n",
        "    return hier_topics_data # Pass this to the plotting function.\n",
        "\n",
        "def plot_topic_visualizations(topic_model, hier_topics_data):\n",
        "    \"\"\"Generates and saves all specified plot visualizations.\"\"\"\n",
        "    # Plot 1: Topic Frequencies Barchart\n",
        "    if len(topic_model.get_topic_info()) > 1:\n",
        "        print(\"\\nGenerating topic frequency barchart...\")\n",
        "        try:\n",
        "            fig = topic_model.visualize_barchart(custom_labels=True, top_n_topics=len(topic_model.get_topic_info()))\n",
        "            fig.write_html(SETTINGS['OUTPUT_PLOT_TOPIC_BARCHART'])\n",
        "            print(f\"Saved topic barchart to '{SETTINGS['OUTPUT_PLOT_TOPIC_BARCHART']}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate barchart: {e}\")\n",
        "\n",
        "    # Plot 2: Individual Topic Keyword Plots\n",
        "    print(\"\\nGenerating keyword plots for top topics...\")\n",
        "    top_topics_df = topic_model.get_topic_info()\n",
        "    top_topics_df = top_topics_df[top_topics_df.Topic != -1].head(SETTINGS['PLOT_TOP_N_TOPICS_KEYWORDS'])\n",
        "    for topic_id in top_topics_df[\"Topic\"]:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        topic_model.visualize_barchart(topics=[topic_id], custom_labels=True)\n",
        "        output_filename = f\"{SETTINGS['OUTPUT_PLOT_KEYWORDS_PREFIX']}_{topic_id}.png\"\n",
        "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    print(f\"Saved top keyword plots with prefix '{SETTINGS['OUTPUT_PLOT_KEYWORDS_PREFIX']}'\")\n",
        "\n",
        "    # Plot 3: 2D Topic Visualization\n",
        "    print(\"\\nGenerating 2D topic visualization...\")\n",
        "    try:\n",
        "        fig_2d = topic_model.visualize_topics()\n",
        "        fig_2d.write_html(SETTINGS['OUTPUT_PLOT_TOPICS_2D'])\n",
        "        print(f\"Saved 2D topic visualization to '{SETTINGS['OUTPUT_PLOT_TOPICS_2D']}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate 2D topic visualization: {e}\")\n",
        "\n",
        "    # Plot 4: Hierarchical Topic Visualization\n",
        "    if hier_topics_data is not None:\n",
        "        print(\"\\nGenerating hierarchical topic visualization...\")\n",
        "        try:\n",
        "            fig_hier = topic_model.visualize_hierarchy(hierarchical_topics=hier_topics_data, custom_labels=True)\n",
        "            fig_hier.write_html(SETTINGS['OUTPUT_PLOT_TOPIC_HIERARCHY'])\n",
        "            print(f\"Saved topic hierarchy visualization to '{SETTINGS['OUTPUT_PLOT_TOPIC_HIERARCHY']}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate topic hierarchy visualization: {e}\")\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "print(\"Please upload your .ris files using the button below.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"\\n⚠️ No files were uploaded. Please run the cell again to upload files.\")\n",
        "else:\n",
        "    print(f\"\\nSuccessfully uploaded {len(uploaded)} file(s). Starting analysis...\")\n",
        "    sentences, original_abstracts = collect_all_ris_sentences(uploaded)\n",
        "\n",
        "    if not sentences:\n",
        "        print(\"\\n❌ Analysis stopped: No sentences could be extracted from the abstracts of the uploaded files.\")\n",
        "    else:\n",
        "        # Run the core topic modeling\n",
        "        topic_model, topics, probs = run_topic_modelling(sentences, original_abstracts)\n",
        "\n",
        "        # Generate labels, tables, and visualizations\n",
        "        generate_custom_topic_labels(topic_model)\n",
        "        hier_data = generate_summary_and_hierarchy_tables(topic_model, sentences)\n",
        "        plot_topic_visualizations(topic_model, hier_data)\n",
        "\n",
        "        print(\"\\n\\n✅ Analysis complete. All output files have been generated.\")\n",
        "        print(\"➡️ Proceed to the final cell to download all results as a single .zip file.\")"
      ],
      "metadata": {
        "id": "3-Sr0vS3XgJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Download All Results\n",
        "# @markdown Run this cell to package all generated files into a single `.zip` archive and download it. <br> This final cell gathers all the output files and compresses them into a single .zip archive. It then automatically triggers a download of this file to your local machine, providing a convenient way to save all results from the Colab session.\n",
        "\n",
        "# Define the name for the output zip file\n",
        "zip_filename = \"bertopic_results.zip\"\n",
        "\n",
        "# --- Gather the list of files to be zipped ---\n",
        "\n",
        "# Start with the core output files defined in SETTINGS\n",
        "files_to_zip = [\n",
        "    SETTINGS[\"OUTPUT_CSV_TOPICS\"],\n",
        "    SETTINGS[\"OUTPUT_CSV_SUMMARY\"],\n",
        "    SETTINGS[\"OUTPUT_PLOT_TOPIC_BARCHART\"],\n",
        "    SETTINGS[\"OUTPUT_PLOT_TOPICS_2D\"],\n",
        "]\n",
        "\n",
        "# Add files that are created conditionally\n",
        "if SETTINGS[\"AUTO_REDUCE_TOPICS\"]:\n",
        "    files_to_zip.append(SETTINGS[\"OUTPUT_PLOT_TOPIC_HIERARCHY\"])\n",
        "    files_to_zip.append(SETTINGS[\"OUTPUT_CSV_HIERARCHY\"])\n",
        "\n",
        "# The outlier CSV is only created if outliers were present initially\n",
        "if os.path.exists(SETTINGS[\"OUTPUT_CSV_OUTLIERS\"]):\n",
        "    files_to_zip.append(SETTINGS[\"OUTPUT_CSV_OUTLIERS\"])\n",
        "\n",
        "# Use glob to find all generated keyword plots that match the prefix\n",
        "keyword_plots = glob.glob(f\"{SETTINGS['OUTPUT_PLOT_KEYWORDS_PREFIX']}_*.png\")\n",
        "files_to_zip.extend(keyword_plots)\n",
        "\n",
        "# --- Create the zip archive ---\n",
        "\n",
        "print(f\"Archiving the following files into '{zip_filename}':\")\n",
        "# Use a with statement to ensure the zip file is properly closed\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add all individual files\n",
        "    for file in files_to_zip:\n",
        "        if os.path.exists(file):\n",
        "            zipf.write(file)\n",
        "            print(f\"- Added: {file}\")\n",
        "        else:\n",
        "            print(f\"- SKIPPED (not found): {file}\")\n",
        "\n",
        "    # Special handling for the saved model directory\n",
        "    model_path = SETTINGS['OUTPUT_MODEL_PATH']\n",
        "    if os.path.isdir(model_path):\n",
        "        print(f\"- Adding model directory: {model_path}\")\n",
        "        # Walk through the model directory and add all its contents\n",
        "        for root, dirs, filenames in os.walk(model_path):\n",
        "            for filename in filenames:\n",
        "                filepath = os.path.join(root, filename)\n",
        "                # The arcname parameter ensures files are stored in the zip with a relative path\n",
        "                arcname = os.path.relpath(filepath, start=os.path.dirname(model_path))\n",
        "                zipf.write(filepath, arcname=arcname)\n",
        "                print(f\"  - Added model file: {arcname}\")\n",
        "    else:\n",
        "        print(f\"- SKIPPED (model directory not found): {model_path}\")\n",
        "\n",
        "print(\"\\n✅ Archiving complete.\")\n",
        "\n",
        "# --- Trigger the download in Google Colab ---\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "OQmihZKFbV6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}